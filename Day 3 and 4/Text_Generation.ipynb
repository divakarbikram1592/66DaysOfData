{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayur7garg/66DaysOfData/blob/main/Day%203/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Generation using LSTM neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h0SYfVAqHcaM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1no4YUq8HmwY"
      },
      "outputs": [],
      "source": [
        "BOOKS = ['The Adventures of Sherlock Holmes by Arthur Conan Doyle.txt',\r\n",
        "         'The Memoirs of Sherlock Holmes by Arthur Conan Doyle.txt',\r\n",
        "         'The Return of Sherlock Holmes by Arthur Conan Doyle.txt']\r\n",
        "\r\n",
        "BASE_PATH = r'../Data/Text/'\r\n",
        "SEQ_LEN = 128\r\n",
        "RANDOM_STATE = 7\r\n",
        "VAL_SIZE = 0.05\r\n",
        "EPOCHS = 50\r\n",
        "BATCH_SIZE = 512\r\n",
        "LEARNING_RATE = 0.001\r\n",
        "EARLY_STOP_PATIENCE = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the text files and creating sequences of fixed length with next character as target label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6w-QjADpMPMB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: 'resent a more dreadful record of sin than does the smiling and\\nbeautiful countryside.”\\n\\n“you horrify me!”\\n\\n“but the reason is ve'\n",
            "Output: r\n",
            "\n",
            "Input: 'ord holdhurst, with a wry face.\\n\\n      “since nearly ten weeks have elapsed, then, and nothing has been\\n      heard, it is not u'\n",
            "Output: n\n",
            "\n",
            "Input: 'your\\ninferences.”\\n\\n“then, pray tell me what it is that you can infer from this hat?”\\n\\nhe picked it up and gazed at it in the pec'\n",
            "Output: u\n",
            "\n",
            "Wall time: 986 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\r\n",
        "\r\n",
        "X = []\r\n",
        "y = []\r\n",
        "\r\n",
        "for book in BOOKS:\r\n",
        "    with open(BASE_PATH + book, 'r', encoding='utf-8') as book_file:\r\n",
        "        book_data = book_file.read().lower()\r\n",
        "        char_len = len(book_data)\r\n",
        "\r\n",
        "        for i in range(0, char_len - SEQ_LEN):\r\n",
        "            X.append(book_data[i : i + SEQ_LEN])\r\n",
        "            y.append(book_data[i + SEQ_LEN])\r\n",
        "\r\n",
        "for i in np.random.randint(0, len(X), 3):\r\n",
        "    print(f'Input: {X[i]!r}')\r\n",
        "    print(f'Output: {y[i]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kj5bERMdPTr_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1863544, 1863544)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X), len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the data for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P9zfhbpTPtRn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1770366, 1770366, 93178, 93178)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = VAL_SIZE, random_state = RANDOM_STATE)\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fitting a tokenizer on training data to create integer encoded sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WMkA8V4rKdF6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found %s unique characters: 79\n",
            "\n",
            "Wall time: 50.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = Tokenizer(char_level = True)\n",
        "tokenizer.fit_on_texts([*X_train, *y_train])\n",
        "char_index = tokenizer.word_index\n",
        "char_count = len(char_index)\n",
        "print(f'Found %s unique characters: {char_count}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integer encoding and scaling the input sequences and one hot encoding the target labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Uxd76cQqKd6W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of input data: \n",
            "Train - (1770366, 128, 1)\n",
            "Validation - (93178, 128, 1)\n",
            "\n",
            "Shape of output data: \n",
            "Train - (1770366, 79)\n",
            "Validation - (93178, 79)\n",
            "\n",
            "Wall time: 58.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_tokenized = np.reshape(X_train_tokenized, (len(X_train_tokenized), SEQ_LEN, 1))\n",
        "X_train_tokenized = X_train_tokenized/char_count\n",
        "\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_tokenized = np.reshape(X_test_tokenized, (len(X_test_tokenized), SEQ_LEN, 1))\n",
        "X_test_tokenized = X_test_tokenized/char_count\n",
        "\n",
        "y_train_categorical = tf.keras.utils.to_categorical(tokenizer.texts_to_sequences(y_train), num_classes = char_count)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(tokenizer.texts_to_sequences(y_test), num_classes = char_count)\n",
        "\n",
        "print(f\"Shape of input data: \\nTrain - {X_train_tokenized.shape}\\nValidation - {X_test_tokenized.shape}\\n\")\n",
        "print(f\"Shape of output data: \\nTrain - {y_train_categorical.shape}\\nValidation - {y_test_categorical.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample input and output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v9f6YOKEUI1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [[0.17721519]\n",
            " [0.34177215]\n",
            " [0.05063291]\n",
            " [0.01265823]\n",
            " [0.10126582]\n",
            " [0.06329114]\n",
            " [0.16455696]\n",
            " [0.11392405]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.06329114]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.24050633]\n",
            " [0.07594937]\n",
            " [0.12658228]\n",
            " [0.02531646]\n",
            " [0.39240506]\n",
            " [0.35443038]\n",
            " [0.01265823]\n",
            " [0.05063291]\n",
            " [0.11392405]\n",
            " [0.32911392]\n",
            " [0.02531646]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.29113924]\n",
            " [0.12658228]\n",
            " [0.05063291]\n",
            " [0.13924051]\n",
            " [0.11392405]\n",
            " [0.03797468]\n",
            " [0.12658228]\n",
            " [0.02531646]\n",
            " [0.02531646]\n",
            " [0.03797468]\n",
            " [0.01265823]\n",
            " [0.05063291]\n",
            " [0.11392405]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.12658228]\n",
            " [0.05063291]\n",
            " [0.07594937]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.11392405]\n",
            " [0.03797468]\n",
            " [0.02531646]\n",
            " [0.05063291]\n",
            " [0.18987342]\n",
            " [0.02531646]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.06329114]\n",
            " [0.24050633]\n",
            " [0.24050633]\n",
            " [0.01265823]\n",
            " [0.05063291]\n",
            " [0.25316456]\n",
            " [0.05063291]\n",
            " [0.07594937]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.06329114]\n",
            " [0.08860759]\n",
            " [0.17721519]\n",
            " [0.07594937]\n",
            " [0.03797468]\n",
            " [0.11392405]\n",
            " [0.01265823]\n",
            " [0.20253165]\n",
            " [0.05063291]\n",
            " [0.2278481 ]\n",
            " [0.30379747]\n",
            " [0.17721519]\n",
            " [0.17721519]\n",
            " [0.34177215]\n",
            " [0.2278481 ]\n",
            " [0.02531646]\n",
            " [0.11392405]\n",
            " [0.26582278]\n",
            " [0.01265823]\n",
            " [0.11392405]\n",
            " [0.07594937]\n",
            " [0.12658228]\n",
            " [0.4556962 ]\n",
            " [0.35443038]\n",
            " [0.01265823]\n",
            " [0.11392405]\n",
            " [0.05063291]\n",
            " [0.07594937]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.11392405]\n",
            " [0.03797468]\n",
            " [0.05063291]\n",
            " [0.03797468]\n",
            " [0.07594937]\n",
            " [0.06329114]\n",
            " [0.08860759]\n",
            " [0.37974684]\n",
            " [0.18987342]\n",
            " [0.05063291]\n",
            " [0.11392405]\n",
            " [0.03797468]\n",
            " [0.02531646]\n",
            " [0.12658228]\n",
            " [0.30379747]\n",
            " [0.17721519]\n",
            " [0.17721519]\n",
            " [0.34177215]\n",
            " [0.20253165]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.13924051]\n",
            " [0.07594937]\n",
            " [0.13924051]]\n",
            "Output: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Input: [[0.03797468]\n",
            " [0.05063291]\n",
            " [0.08860759]\n",
            " [0.03797468]\n",
            " [0.01265823]\n",
            " [0.15189873]\n",
            " [0.05063291]\n",
            " [0.03797468]\n",
            " [0.02531646]\n",
            " [0.12658228]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.18987342]\n",
            " [0.05063291]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.02531646]\n",
            " [0.18987342]\n",
            " [0.02531646]\n",
            " [0.12658228]\n",
            " [0.25316456]\n",
            " [0.02531646]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.24050633]\n",
            " [0.12658228]\n",
            " [0.06329114]\n",
            " [0.18987342]\n",
            " [0.01265823]\n",
            " [0.10126582]\n",
            " [0.07594937]\n",
            " [0.11392405]\n",
            " [0.01265823]\n",
            " [0.10126582]\n",
            " [0.07594937]\n",
            " [0.13924051]\n",
            " [0.07594937]\n",
            " [0.08860759]\n",
            " [0.25316456]\n",
            " [0.37974684]\n",
            " [0.27848101]\n",
            " [0.15189873]\n",
            " [0.05063291]\n",
            " [0.21518987]\n",
            " [0.02531646]\n",
            " [0.26582278]\n",
            " [0.01265823]\n",
            " [0.11392405]\n",
            " [0.27848101]\n",
            " [0.12658228]\n",
            " [0.05063291]\n",
            " [0.08860759]\n",
            " [0.25316456]\n",
            " [0.01265823]\n",
            " [0.16455696]\n",
            " [0.27848101]\n",
            " [0.06329114]\n",
            " [0.08860759]\n",
            " [0.17721519]\n",
            " [0.01265823]\n",
            " [0.01265823]\n",
            " [0.01265823]\n",
            " [0.01265823]\n",
            " [0.01265823]\n",
            " [0.01265823]\n",
            " [0.10126582]\n",
            " [0.07594937]\n",
            " [0.11392405]\n",
            " [0.01265823]\n",
            " [0.21518987]\n",
            " [0.2278481 ]\n",
            " [0.21518987]\n",
            " [0.15189873]\n",
            " [0.02531646]\n",
            " [0.26582278]\n",
            " [0.01265823]\n",
            " [0.05063291]\n",
            " [0.08860759]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.24050633]\n",
            " [0.06329114]\n",
            " [0.15189873]\n",
            " [0.15189873]\n",
            " [0.06329114]\n",
            " [0.20253165]\n",
            " [0.02531646]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.12658228]\n",
            " [0.30379747]\n",
            " [0.01265823]\n",
            " [0.07594937]\n",
            " [0.08860759]\n",
            " [0.01265823]\n",
            " [0.05063291]\n",
            " [0.15189873]\n",
            " [0.15189873]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.10126582]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.29113924]\n",
            " [0.12658228]\n",
            " [0.06329114]\n",
            " [0.05063291]\n",
            " [0.13924051]\n",
            " [0.01265823]\n",
            " [0.15189873]\n",
            " [0.05063291]\n",
            " [0.08860759]\n",
            " [0.13924051]\n",
            " [0.11392405]\n",
            " [0.21518987]\n",
            " [0.05063291]\n",
            " [0.27848101]\n",
            " [0.02531646]\n",
            " [0.01265823]\n",
            " [0.03797468]\n",
            " [0.10126582]\n",
            " [0.06329114]\n",
            " [0.11392405]]\n",
            "Output: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in np.random.randint(0, len(X_train_tokenized), 2):\n",
        "    print(f'Input: {X_train_tokenized[i]}')\n",
        "    print(f'Output: {y_train_categorical[i]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating an LSTM model using tf.keras Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0khGlIxbXUv6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Text_Generation_Model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128, 512)          1052672   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               787456    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 79)                10191     \n",
            "=================================================================\n",
            "Total params: 1,949,007\n",
            "Trainable params: 1,949,007\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\r\n",
        "    LSTM(512, return_sequences = True,  input_shape=(SEQ_LEN, 1)),\r\n",
        "    Dropout(0.05),\r\n",
        "    LSTM(256),\r\n",
        "    Dense(256, activation = LeakyReLU()),\r\n",
        "    Dropout(0.05),\r\n",
        "    Dense(128, activation = LeakyReLU()),\r\n",
        "    Dense(char_count, activation = 'softmax')\r\n",
        "    ], name = 'Text_Generation_Model')\r\n",
        "\r\n",
        "model.compile(optimizer = Adam(LEARNING_RATE), loss='categorical_crossentropy', metrics = ['categorical_accuracy'])\r\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model with EarlyStopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XN7_hxljYXpo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3458/3458 [==============================] - 1466s 424ms/step - loss: 2.6278 - categorical_accuracy: 0.2768 - val_loss: 2.1638 - val_categorical_accuracy: 0.3849\n",
            "Epoch 2/50\n",
            "3458/3458 [==============================] - 1466s 424ms/step - loss: 1.9568 - categorical_accuracy: 0.4340 - val_loss: 1.7679 - val_categorical_accuracy: 0.4810\n",
            "Epoch 3/50\n",
            "3458/3458 [==============================] - 1465s 424ms/step - loss: 1.7098 - categorical_accuracy: 0.4986 - val_loss: 1.6083 - val_categorical_accuracy: 0.5265\n",
            "Epoch 4/50\n",
            "3458/3458 [==============================] - 1465s 424ms/step - loss: 1.5814 - categorical_accuracy: 0.5326 - val_loss: 1.5142 - val_categorical_accuracy: 0.5522\n",
            "Epoch 5/50\n",
            "3458/3458 [==============================] - 1465s 424ms/step - loss: 1.5005 - categorical_accuracy: 0.5537 - val_loss: 1.4569 - val_categorical_accuracy: 0.5666\n",
            "Epoch 6/50\n",
            "3458/3458 [==============================] - 1483s 429ms/step - loss: 1.4438 - categorical_accuracy: 0.5683 - val_loss: 1.4136 - val_categorical_accuracy: 0.5766\n",
            "Epoch 7/50\n",
            "3458/3458 [==============================] - 1469s 425ms/step - loss: 1.4002 - categorical_accuracy: 0.5791 - val_loss: 1.3757 - val_categorical_accuracy: 0.5852\n",
            "Epoch 8/50\n",
            "3458/3458 [==============================] - 1470s 425ms/step - loss: 1.3654 - categorical_accuracy: 0.5876 - val_loss: 1.3475 - val_categorical_accuracy: 0.5928\n",
            "Epoch 9/50\n",
            "3458/3458 [==============================] - 1477s 427ms/step - loss: 1.3345 - categorical_accuracy: 0.5949 - val_loss: 1.3318 - val_categorical_accuracy: 0.5973\n",
            "Epoch 10/50\n",
            "3458/3458 [==============================] - 1464s 423ms/step - loss: 1.3102 - categorical_accuracy: 0.6006 - val_loss: 1.3075 - val_categorical_accuracy: 0.6014\n",
            "Epoch 11/50\n",
            "3458/3458 [==============================] - 1477s 427ms/step - loss: 1.2890 - categorical_accuracy: 0.6066 - val_loss: 1.2901 - val_categorical_accuracy: 0.6079\n",
            "Epoch 12/50\n",
            "3458/3458 [==============================] - 1464s 423ms/step - loss: 1.2712 - categorical_accuracy: 0.6113 - val_loss: 1.2717 - val_categorical_accuracy: 0.6135\n",
            "Epoch 13/50\n",
            "3458/3458 [==============================] - 1461s 423ms/step - loss: 1.2560 - categorical_accuracy: 0.6152 - val_loss: 1.2657 - val_categorical_accuracy: 0.6139\n",
            "Epoch 14/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.2423 - categorical_accuracy: 0.6190 - val_loss: 1.2629 - val_categorical_accuracy: 0.6151\n",
            "Epoch 15/50\n",
            "3458/3458 [==============================] - 1460s 422ms/step - loss: 1.2291 - categorical_accuracy: 0.6222 - val_loss: 1.2557 - val_categorical_accuracy: 0.6166\n",
            "Epoch 16/50\n",
            "3458/3458 [==============================] - 1460s 422ms/step - loss: 1.2178 - categorical_accuracy: 0.6250 - val_loss: 1.2417 - val_categorical_accuracy: 0.6210\n",
            "Epoch 17/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.2085 - categorical_accuracy: 0.6277 - val_loss: 1.2261 - val_categorical_accuracy: 0.6247\n",
            "Epoch 18/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1996 - categorical_accuracy: 0.6296 - val_loss: 1.2336 - val_categorical_accuracy: 0.6251\n",
            "Epoch 19/50\n",
            "3458/3458 [==============================] - 1460s 422ms/step - loss: 1.1907 - categorical_accuracy: 0.6322 - val_loss: 1.2200 - val_categorical_accuracy: 0.6262\n",
            "Epoch 20/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1831 - categorical_accuracy: 0.6341 - val_loss: 1.2256 - val_categorical_accuracy: 0.6262\n",
            "Epoch 21/50\n",
            "3458/3458 [==============================] - 1458s 422ms/step - loss: 1.1759 - categorical_accuracy: 0.6357 - val_loss: 1.2080 - val_categorical_accuracy: 0.6293\n",
            "Epoch 22/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1694 - categorical_accuracy: 0.6375 - val_loss: 1.2119 - val_categorical_accuracy: 0.6297\n",
            "Epoch 23/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1627 - categorical_accuracy: 0.6391 - val_loss: 1.2040 - val_categorical_accuracy: 0.6303\n",
            "Epoch 24/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1566 - categorical_accuracy: 0.6407 - val_loss: 1.2131 - val_categorical_accuracy: 0.6292\n",
            "Epoch 25/50\n",
            "3458/3458 [==============================] - 1459s 422ms/step - loss: 1.1511 - categorical_accuracy: 0.6421 - val_loss: 1.1962 - val_categorical_accuracy: 0.6346\n",
            "Epoch 26/50\n",
            "3458/3458 [==============================] - 1460s 422ms/step - loss: 1.1456 - categorical_accuracy: 0.6434 - val_loss: 1.1920 - val_categorical_accuracy: 0.6347\n",
            "Epoch 27/50\n",
            "3458/3458 [==============================] - 1461s 422ms/step - loss: 1.1401 - categorical_accuracy: 0.6452 - val_loss: 1.1962 - val_categorical_accuracy: 0.6335\n",
            "Epoch 28/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.1353 - categorical_accuracy: 0.6462 - val_loss: 1.1891 - val_categorical_accuracy: 0.6357\n",
            "Epoch 29/50\n",
            "3458/3458 [==============================] - 1460s 422ms/step - loss: 1.1310 - categorical_accuracy: 0.6474 - val_loss: 1.1824 - val_categorical_accuracy: 0.6366\n",
            "Epoch 30/50\n",
            "3458/3458 [==============================] - 1461s 422ms/step - loss: 1.1263 - categorical_accuracy: 0.6483 - val_loss: 1.1785 - val_categorical_accuracy: 0.6390\n",
            "Epoch 31/50\n",
            "3458/3458 [==============================] - 1461s 423ms/step - loss: 1.1220 - categorical_accuracy: 0.6496 - val_loss: 1.1825 - val_categorical_accuracy: 0.6380\n",
            "Epoch 32/50\n",
            "3458/3458 [==============================] - 1461s 423ms/step - loss: 1.1177 - categorical_accuracy: 0.6509 - val_loss: 1.1757 - val_categorical_accuracy: 0.6398\n",
            "Epoch 33/50\n",
            "3458/3458 [==============================] - 1461s 422ms/step - loss: 1.1135 - categorical_accuracy: 0.6520 - val_loss: 1.1797 - val_categorical_accuracy: 0.6393\n",
            "Epoch 34/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.1098 - categorical_accuracy: 0.6525 - val_loss: 1.1783 - val_categorical_accuracy: 0.6384\n",
            "Epoch 35/50\n",
            "3458/3458 [==============================] - 1461s 422ms/step - loss: 1.1058 - categorical_accuracy: 0.6542 - val_loss: 1.1703 - val_categorical_accuracy: 0.6404\n",
            "Epoch 36/50\n",
            "3458/3458 [==============================] - 1463s 423ms/step - loss: 1.1017 - categorical_accuracy: 0.6553 - val_loss: 1.1734 - val_categorical_accuracy: 0.6405\n",
            "Epoch 37/50\n",
            "3458/3458 [==============================] - 1463s 423ms/step - loss: 1.0990 - categorical_accuracy: 0.6556 - val_loss: 1.1704 - val_categorical_accuracy: 0.6422\n",
            "Epoch 38/50\n",
            "3458/3458 [==============================] - 1465s 424ms/step - loss: 1.0956 - categorical_accuracy: 0.6565 - val_loss: 1.1751 - val_categorical_accuracy: 0.6400\n",
            "Epoch 39/50\n",
            "3458/3458 [==============================] - 1465s 424ms/step - loss: 1.0915 - categorical_accuracy: 0.6578 - val_loss: 1.1665 - val_categorical_accuracy: 0.6426\n",
            "Epoch 40/50\n",
            "3458/3458 [==============================] - 1464s 423ms/step - loss: 1.0885 - categorical_accuracy: 0.6588 - val_loss: 1.1643 - val_categorical_accuracy: 0.6435\n",
            "Epoch 41/50\n",
            "3458/3458 [==============================] - 1463s 423ms/step - loss: 1.0851 - categorical_accuracy: 0.6594 - val_loss: 1.1760 - val_categorical_accuracy: 0.6413\n",
            "Epoch 42/50\n",
            "3458/3458 [==============================] - 1463s 423ms/step - loss: 1.0824 - categorical_accuracy: 0.6603 - val_loss: 1.1669 - val_categorical_accuracy: 0.6431\n",
            "Epoch 43/50\n",
            "3458/3458 [==============================] - 1463s 423ms/step - loss: 1.0793 - categorical_accuracy: 0.6610 - val_loss: 1.1641 - val_categorical_accuracy: 0.6430\n",
            "Epoch 44/50\n",
            "3458/3458 [==============================] - 1464s 423ms/step - loss: 1.0766 - categorical_accuracy: 0.6618 - val_loss: 1.1658 - val_categorical_accuracy: 0.6434\n",
            "Epoch 45/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.0736 - categorical_accuracy: 0.6624 - val_loss: 1.1798 - val_categorical_accuracy: 0.6384\n",
            "Epoch 46/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.0705 - categorical_accuracy: 0.6635 - val_loss: 1.1690 - val_categorical_accuracy: 0.6437\n",
            "Epoch 47/50\n",
            "3458/3458 [==============================] - 1461s 423ms/step - loss: 1.0678 - categorical_accuracy: 0.6639 - val_loss: 1.1640 - val_categorical_accuracy: 0.6441\n",
            "Epoch 48/50\n",
            "3458/3458 [==============================] - 1462s 423ms/step - loss: 1.0657 - categorical_accuracy: 0.6647 - val_loss: 1.1666 - val_categorical_accuracy: 0.6430\n",
            "Epoch 49/50\n",
            "3458/3458 [==============================] - 1589s 460ms/step - loss: 1.0630 - categorical_accuracy: 0.6655 - val_loss: 1.1622 - val_categorical_accuracy: 0.6440\n",
            "Epoch 50/50\n",
            "3458/3458 [==============================] - 1472s 426ms/step - loss: 1.0594 - categorical_accuracy: 0.6665 - val_loss: 1.1562 - val_categorical_accuracy: 0.6469\n",
            "Wall time: 20h 21min 58s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE, restore_best_weights = True)\n",
        "history = model.fit(\n",
        "    X_train_tokenized, \n",
        "    y_train_categorical, \n",
        "    epochs = EPOCHS, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    validation_data = (X_test_tokenized, y_test_categorical),\n",
        "    callbacks = [early_stop]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a reverse mapping dictionary for encoded integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QEBq1I74YbNW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       1 :  \n",
            "       2 : e\n",
            "       3 : t\n",
            "       4 : a\n",
            "       5 : o\n",
            "       6 : i\n",
            "       7 : n\n",
            "       8 : h\n",
            "       9 : s\n",
            "      10 : r\n",
            "      11 : d\n",
            "      12 : l\n",
            "      13 : u\n",
            "      14 : \n",
            "\n",
            "      15 : m\n",
            "      16 : w\n",
            "      17 : c\n",
            "      18 : y\n",
            "      19 : f\n",
            "      20 : g\n",
            "      21 : ,\n",
            "      22 : p\n",
            "      23 : b\n",
            "      24 : .\n",
            "      25 : v\n",
            "      26 : k\n",
            "      27 : “\n",
            "      28 : ”\n",
            "      29 : ’\n",
            "      30 : -\n",
            "      31 : ?\n",
            "      32 : x\n",
            "      33 : j\n",
            "      34 : q\n",
            "      35 : ‘\n",
            "      36 : !\n",
            "      37 : —\n",
            "      38 : z\n",
            "      39 : _\n",
            "      40 : ;\n",
            "      41 : 1\n",
            "      42 : :\n",
            "      43 : 0\n",
            "      44 : 8\n",
            "      45 : 2\n",
            "      46 : 3\n",
            "      47 : *\n",
            "      48 : )\n",
            "      49 : (\n",
            "      50 : 4\n",
            "      51 : 5\n",
            "      52 : 9\n",
            "      53 : 6\n",
            "      54 : /\n",
            "      55 : 7\n",
            "      56 : £\n",
            "      57 : é\n",
            "      58 : \"\n",
            "      59 : &\n",
            "      60 : '\n",
            "      61 : æ\n",
            "      62 : \t\n",
            "      63 : $\n",
            "      64 : @\n",
            "      65 : œ\n",
            "      66 : [\n",
            "      67 : º\n",
            "      68 : ]\n",
            "      69 : #\n",
            "      70 : %\n",
            "      71 : è\n",
            "      72 : ・\n",
            "      73 : â\n",
            "      74 : à\n",
            "      75 : ô\n",
            "      76 : ï\n",
            "      77 : î\n",
            "      78 : ½\n",
            "      79 : ﻿\n"
          ]
        }
      ],
      "source": [
        "index_char = {ind: char for char, ind in char_index.items()}\r\n",
        "for k, v in index_char.items():\r\n",
        "    print(f\"{k:8} : {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definite prediction\r\n",
        "For the seed string provided, the character with the highest predicted probability is selected. The seed is then updated with the predicted character and this process repeats for a specified number of times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            "he exception of his coat. his\n",
            "boots, his socks, his hat, and his watch—all were there. there were no\n",
            "signs of violence upon any \n",
            "Output:\n",
            "attempt to the station and the station as\n",
            "the bell-rope to the room and the station to the room which i had a bearing\n",
            "to the room and the start of the corner of the colonel and the station\n",
            "and the room was a little start of the corner of the corner of the\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def definite(seed, pred_count = 256):\r\n",
        "    print(f\"Input:\\n{seed}\")\r\n",
        "\r\n",
        "    for i in range(pred_count):\r\n",
        "        input_data = tokenizer.texts_to_sequences(seed[i: i + SEQ_LEN])\r\n",
        "        input_data = np.reshape(input_data, (1, SEQ_LEN, 1))\r\n",
        "        input_data = input_data/char_count\r\n",
        "        pred_char = index_char[np.argmax(model.predict(input_data))]\r\n",
        "        seed += pred_char\r\n",
        "\r\n",
        "    print(f\"Output:\\n{seed[SEQ_LEN:]}\")\r\n",
        "\r\n",
        "seed = X_test[np.random.randint(0, len(X_test))]\r\n",
        "definite(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Probabilistic prediction\r\n",
        "For the seed string provided, the character is selected randomly weighted by the probabilities predicted by the model. The seed is then updated with the predicted character and this process repeats for a specified number of times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " in not making an effort. look at this!” he held up a\n",
            "      little note with a coat-of-arms upon the envelope. “that belongs\n",
            "   \n",
            "Output:\n",
            "   it was. i intend the three-quarter headounde of cross—fest rustice\n",
            "      of young mycersing evil repulbing more, epes an evidence. in it\n",
            "      was a month went any adgancy of the breakfast that i soon come,\n",
            "      i care me on so i asked me to fild a wea\n"
          ]
        }
      ],
      "source": [
        "def probabilistic(seed, pred_count = 256):\r\n",
        "    print(f\"Input:\\n{seed}\")\r\n",
        "\r\n",
        "    for i in range(pred_count):\r\n",
        "        input_data = tokenizer.texts_to_sequences(seed[i: i + SEQ_LEN])\r\n",
        "        input_data = np.reshape(input_data, (1, SEQ_LEN, 1))\r\n",
        "        input_data = input_data/char_count\r\n",
        "        pred_prob = model.predict(input_data).reshape(-1)\r\n",
        "        pred_char = index_char[np.random.choice(len(pred_prob), p = pred_prob)]\r\n",
        "        seed += pred_char\r\n",
        "\r\n",
        "    print(f\"Output:\\n{seed[SEQ_LEN:]}\")\r\n",
        "\r\n",
        "seed = X_test[np.random.randint(0, len(X_test))]\r\n",
        "probabilistic(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOKUycO4o3WThrQcJINp1Da",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Text Generation.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "dcbe076a40d8142e585077643c26fc4a9c0eed423ce3f041c8a5b2e5c8137bb1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}