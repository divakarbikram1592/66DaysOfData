{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKUycO4o3WThrQcJINp1Da",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayur7garg/66DaysOfData/blob/main/Day%203/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0SYfVAqHcaM"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1no4YUq8HmwY"
      },
      "source": [
        "BOOKS = ['The Adventures of Sherlock Holmes by Arthur Conan Doyle.txt',\n",
        "         'The Memoirs of Sherlock Holmes by Arthur Conan Doyle.txt',\n",
        "         'The Return of Sherlock Holmes by Arthur Conan Doyle.txt']\n",
        "\n",
        "BASE_PATH = r'/content/'\n",
        "SEQ_LEN = 128\n",
        "RANDOM_STATE = 7\n",
        "VAL_SIZE = 0.05\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 512\n",
        "LEARNING_RATE = 0.01\n",
        "EARLY_STOP_PATIENCE = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w-QjADpMPMB"
      },
      "source": [
        "%%time\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for book in BOOKS:\n",
        "    with open(BASE_PATH + book, 'r') as book_file:\n",
        "        book_data = book_file.read().lower()\n",
        "        char_len = len(book_data)\n",
        "\n",
        "        for i in range(0, char_len - SEQ_LEN):\n",
        "            X.append(book_data[i : i + SEQ_LEN])\n",
        "            y.append(book_data[i + SEQ_LEN])\n",
        "\n",
        "for i in np.random.randint(0, len(X), 3):\n",
        "    print(f'Input: {X[i]!r}')\n",
        "    print(f'Output: {y[i]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj5bERMdPTr_"
      },
      "source": [
        "len(X), len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9zfhbpTPtRn"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = VAL_SIZE, random_state = RANDOM_STATE)\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMkA8V4rKdF6"
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = Tokenizer(char_level = True)\n",
        "tokenizer.fit_on_texts([*X_train, *y_train])\n",
        "char_index = tokenizer.word_index\n",
        "char_count = len(char_index)\n",
        "print(f'Found %s unique characters: {char_count}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxd76cQqKd6W"
      },
      "source": [
        "%%time\n",
        "\n",
        "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_tokenized = np.reshape(X_train_tokenized, (len(X_train_tokenized), SEQ_LEN, 1))\n",
        "X_train_tokenized = X_train_tokenized/char_count\n",
        "\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_tokenized = np.reshape(X_test_tokenized, (len(X_test_tokenized), SEQ_LEN, 1))\n",
        "X_test_tokenized = X_test_tokenized/char_count\n",
        "\n",
        "y_train_categorical = tf.keras.utils.to_categorical(tokenizer.texts_to_sequences(y_train), num_classes = char_count)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(tokenizer.texts_to_sequences(y_test), num_classes = char_count)\n",
        "\n",
        "print(f\"Shape of input data: \\nTrain - {X_train_tokenized.shape}\\nValidation - {X_test_tokenized.shape}\\n\")\n",
        "print(f\"Shape of output data: \\nTrain - {y_train_categorical.shape}\\nValidation - {y_test_categorical.shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9f6YOKEUI1c"
      },
      "source": [
        "for i in np.random.randint(0, len(X_train_tokenized), 2):\n",
        "    print(f'Input: {X_train_tokenized[i]}')\n",
        "    print(f'Output: {y_train_categorical[i]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0khGlIxbXUv6"
      },
      "source": [
        "model = Sequential([\n",
        "    LSTM((512), return_sequences = True,  input_shape=(SEQ_LEN, 1)),\n",
        "    LSTM((512), return_sequences = True),\n",
        "    LSTM((256), return_sequences = False),\n",
        "    Dropout(0.1),\n",
        "    Dense(256, activation = LeakyReLU()),\n",
        "    Dense(128, activation = LeakyReLU()),\n",
        "    Dropout(0.1),\n",
        "    Dense(char_count, activation = 'softmax')\n",
        "    ], name = 'Text_Generation_Model')\n",
        "\n",
        "model.compile(optimizer = Adam(LEARNING_RATE), loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN7_hxljYXpo"
      },
      "source": [
        "%%time\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE, restore_best_weights = True)\n",
        "history = model.fit(\n",
        "    X_train_tokenized, \n",
        "    y_train_categorical, \n",
        "    epochs = EPOCHS, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    validation_data = (X_test_tokenized, y_test_categorical),\n",
        "    callbacks = [early_stop]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEBq1I74YbNW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}